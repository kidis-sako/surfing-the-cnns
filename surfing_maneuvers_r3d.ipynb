{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Surfing Maneuver Recognition using R3D Transfer Learning\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook demonstrates transfer learning for video-based surfing maneuver recognition using the **R3D (3D ResNet)** model.\n",
        "\n",
        "**Dataset**: Surf Maneuver Recognition Dataset\n",
        "\n",
        "The dataset contains video clips of surfers performing four different maneuvers:\n",
        "1. **Cutback-Frontside**: A sharp turn back towards the breaking part of the wave\n",
        "2. **Take-off**: The act of catching the wave and standing up on the surfboard\n",
        "3. **360**: A full rotation maneuver while on the wave\n",
        "4. **Roller**: Riding on top of the breaking wave\n",
        "\n",
        "**Model**: [R3D](https://arxiv.org/abs/1711.11248) (3D ResNet) is a powerful video recognition model that extends the ResNet architecture to 3D convolutions for capturing spatiotemporal features in videos. It applies 3D convolutions throughout the network to jointly model spatial and temporal information.\n",
        "\n",
        "**Approach**: We'll use transfer learning by loading R3D pretrained on Kinetics-400 (a large action recognition dataset with 400 classes) and fine-tune it for our surfing maneuver classification task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models.video import r3d_18, R3D_18_Weights\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Device Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")  # For Mac M1/M2\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ⚡ Performance Optimization: Cached Dataset\n",
        "\n",
        "**Problem:** Loading and preprocessing videos on-the-fly is very slow:\n",
        "- Video decoding happens for every epoch\n",
        "- Frame extraction and resizing repeated unnecessarily\n",
        "- With `num_workers=0`, this creates a major bottleneck\n",
        "\n",
        "**Solution:** Preprocess videos **once** and cache them as `.pt` files:\n",
        "1. Run `preprocess_videos_cache.py` to create cached tensors\n",
        "2. Use `CachedSurfingManeuverDataset` to load preprocessed data\n",
        "3. Get **10-20x faster** training speed!\n",
        "\n",
        "**Usage:**\n",
        "```bash\n",
        "# First time only - preprocess and cache all videos\n",
        "python preprocess_videos_cache.py\n",
        "```\n",
        "\n",
        "Then use the cached dataset below instead of the regular dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CachedSurfingManeuverDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Fast dataset that loads pre-cached video tensors.\n",
        "    \n",
        "    Videos should be preprocessed using preprocess_videos_cache.py first.\n",
        "    This provides 10-20x faster data loading compared to on-the-fly preprocessing.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, cache_dir):\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        \n",
        "        # Get all cached .pt files\n",
        "        self.cache_files = []\n",
        "        self.labels = []\n",
        "        \n",
        "        for class_idx, class_name in enumerate(class_names):\n",
        "            class_cache_dir = self.cache_dir / class_name\n",
        "            if class_cache_dir.exists():\n",
        "                pt_files = list(class_cache_dir.glob('*.pt'))\n",
        "                self.cache_files.extend(pt_files)\n",
        "                self.labels.extend([class_idx] * len(pt_files))\n",
        "        \n",
        "        print(f\"Found {len(self.cache_files)} cached videos across {len(class_names)} classes\")\n",
        "        \n",
        "        if len(self.cache_files) == 0:\n",
        "            print(f\"\\n⚠️  WARNING: No cached files found in {cache_dir}\")\n",
        "            print(f\"Please run 'python preprocess_videos_cache.py' first to create the cache.\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.cache_files)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Load a preprocessed video tensor from cache.\"\"\"\n",
        "        cache_file = self.cache_files[idx]\n",
        "        \n",
        "        try:\n",
        "            # Load cached data (already preprocessed!)\n",
        "            data = torch.load(cache_file, map_location='cpu')\n",
        "            video = data['video']\n",
        "            label = data['label']\n",
        "            \n",
        "            return video, label\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error loading cached file {cache_file}: {e}\")\n",
        "            # Return a random valid sample instead\n",
        "            return self.__getitem__((idx + 1) % len(self))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Adjust these values as needed\n",
        "BATCH_SIZE = 4  # Smaller batch size for video (memory intensive)\n",
        "EPOCHS = 5\n",
        "NUM_FRAMES = 64  # R3D typically uses 64 frames (can be adjusted)\n",
        "FRAME_SIZE = 224  # R3D input size (for reference - official transforms handle resizing)\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_CLASSES = 4  # Cutback-Frontside, Take-off, 360, Roller\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Set all random seeds for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Make training deterministic (may impact performance slightly)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Create generator for DataLoader\n",
        "dataloader_generator = torch.Generator()\n",
        "dataloader_generator.manual_seed(SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Class Names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_names = [\n",
        "    'cutback-frontside',\n",
        "    'take-off',\n",
        "    '360',\n",
        "    'roller'\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Dataset\n",
        "\n",
        "**Note:** This notebook uses cached preprocessed videos for fast training (10-20x speedup).\n",
        "\n",
        "Before running this cell for the first time, you must preprocess the videos:\n",
        "```bash\n",
        "python preprocess_videos_cache.py\n",
        "```\n",
        "\n",
        "This will create `surfing_dataset_cache/` with preprocessed tensors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Custom Video Dataset\n",
        "\n",
        "Since we're working with video data, we need a custom dataset class to:\n",
        "- Load video files\n",
        "- Sample frames uniformly\n",
        "- Apply transformations\n",
        "- Convert to the format expected by R3D (C, T, H, W)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SurfingManeuverDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset for loading surfing maneuver videos.\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, data_dir, num_frames=13, transform=None):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.num_frames = num_frames\n",
        "        self.transform = transform\n",
        "        \n",
        "        # Get all video files and their labels\n",
        "        self.video_paths = []\n",
        "        self.labels = []\n",
        "        \n",
        "        # Supported video extensions\n",
        "        video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.MP4', '.AVI', '.MOV', '.MKV']\n",
        "        \n",
        "        for class_idx, class_name in enumerate(class_names):\n",
        "            class_dir = self.data_dir / class_name\n",
        "            if class_dir.exists():\n",
        "                for video_file in class_dir.iterdir():\n",
        "                    if video_file.suffix in video_extensions:\n",
        "                        self.video_paths.append(video_file)\n",
        "                        self.labels.append(class_idx)\n",
        "        \n",
        "        print(f\"Found {len(self.video_paths)} videos across {len(class_names)} classes\")\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.video_paths)\n",
        "    \n",
        "    def uniform_sample_indices(self, num_frames, target_frames):\n",
        "        \"\"\"\n",
        "        Uniformly sample frame indices from a video.\n",
        "        \n",
        "        Args:\n",
        "            num_frames: Total number of frames in the video\n",
        "            target_frames: Number of frames to sample\n",
        "            \n",
        "        Returns:\n",
        "            Array of frame indices\n",
        "        \"\"\"\n",
        "        if num_frames < target_frames:\n",
        "            # If video has fewer frames than needed, use linspace which will repeat indices\n",
        "            indices = np.linspace(0, num_frames - 1, target_frames).round().astype(int)\n",
        "        else:\n",
        "            # Uniform sampling across the video\n",
        "            indices = np.linspace(0, num_frames - 1, target_frames).round().astype(int)\n",
        "        \n",
        "        return indices\n",
        "    \n",
        "    def load_video(self, video_path):\n",
        "        \"\"\"\n",
        "        Load video and sample frames uniformly.\n",
        "        Returns: tensor of shape (T, H, W, C) in range [0, 255] as uint8\n",
        "        R3D VideoClassification transforms expect this format.\n",
        "        \"\"\"\n",
        "        cap = cv2.VideoCapture(str(video_path))\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        \n",
        "        if total_frames == 0:\n",
        "            raise ValueError(f\"Could not read video: {video_path}\")\n",
        "        \n",
        "        # Uniform temporal sampling\n",
        "        frame_indices = self.uniform_sample_indices(total_frames, self.num_frames)\n",
        "        \n",
        "        frames = []\n",
        "        for idx in frame_indices:\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "            ret, frame = cap.read()\n",
        "            if ret:\n",
        "                # CRITICAL: Convert BGR to RGB (OpenCV loads as BGR)\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                # NOTE: We don't resize here - let the official transforms handle it\n",
        "                frames.append(frame)\n",
        "            else:\n",
        "                # If frame reading fails, use the last successful frame\n",
        "                if frames:\n",
        "                    frames.append(frames[-1])\n",
        "                else:\n",
        "                    raise ValueError(f\"Could not read frame {idx} from {video_path}\")\n",
        "        \n",
        "        cap.release()\n",
        "        \n",
        "        # Convert to numpy array: (T, H, W, C)\n",
        "        video = np.stack(frames)\n",
        "        \n",
        "        # Convert to tensor, keep as uint8 in [T, H, W, C] format\n",
        "        # R3D VideoClassification transforms expect this format\n",
        "        video = torch.from_numpy(video).to(torch.uint8)\n",
        "        \n",
        "        return video\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        video_path = self.video_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        \n",
        "        try:\n",
        "            video = self.load_video(video_path)\n",
        "            \n",
        "            if self.transform:\n",
        "                video = self.transform(video)\n",
        "            \n",
        "            return video, label\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading video {video_path}: {e}\")\n",
        "            # Return a random valid sample instead\n",
        "            return self.__getitem__((idx + 1) % len(self))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Transforms\n",
        "\n",
        "R3D expects inputs with specific normalization (from Kinetics-400 pretraining).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# R3D preprocessing - using manual transforms since official ones have format issues\n",
        "from torchvision.models.video import R3D_18_Weights\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "# Normalization values from R3D Kinetics-400 pretraining\n",
        "MEAN = [0.43216, 0.394666, 0.37645]\n",
        "STD = [0.22803, 0.22145, 0.216989]\n",
        "\n",
        "def video_transform(video):\n",
        "    \"\"\"\n",
        "    Transform video from [T, H, W, C] uint8 to [C, T, H, W] float32 normalized.\n",
        "    \n",
        "    Args:\n",
        "        video: torch.Tensor of shape [T, H, W, C] with uint8 values [0, 255]\n",
        "    \n",
        "    Returns:\n",
        "        Processed video tensor [C, T, H, W] ready for R3D model\n",
        "    \"\"\"\n",
        "    # Convert from [T, H, W, C] to [C, T, H, W]\n",
        "    video = video.permute(3, 0, 1, 2)  # [T, H, W, C] -> [C, T, H, W]\n",
        "    \n",
        "    # Convert to float and scale to [0, 1]\n",
        "    video = video.float() / 255.0\n",
        "    \n",
        "    # Resize each frame to 256x256 then center crop to 224x224\n",
        "    # Process frame by frame to handle the temporal dimension correctly\n",
        "    T = video.shape[1]\n",
        "    resized_frames = []\n",
        "    for t in range(T):\n",
        "        frame = video[:, t, :, :]  # [C, H, W]\n",
        "        # Resize to 256x256\n",
        "        frame = F.resize(frame, [256, 256], antialias=True)\n",
        "        # Center crop to 224x224\n",
        "        frame = F.center_crop(frame, [224, 224])\n",
        "        resized_frames.append(frame)\n",
        "    \n",
        "    # Stack back to [C, T, H, W]\n",
        "    video = torch.stack(resized_frames, dim=1)\n",
        "    \n",
        "    # Normalize with R3D mean and std\n",
        "    mean = torch.tensor(MEAN).view(3, 1, 1, 1)\n",
        "    std = torch.tensor(STD).view(3, 1, 1, 1)\n",
        "    video = (video - mean) / std\n",
        "    \n",
        "    return video\n",
        "\n",
        "train_transform = video_transform\n",
        "val_transform = video_transform\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cached dataset paths\n",
        "CACHE_TRAIN_DIR = './surfing_dataset_cache/train'\n",
        "CACHE_VAL_DIR = './surfing_dataset_cache/val'\n",
        "CACHE_TEST_DIR = './surfing_dataset_cache/test'\n",
        "\n",
        "# Load cached datasets\n",
        "train_dataset = CachedSurfingManeuverDataset(cache_dir=CACHE_TRAIN_DIR)\n",
        "val_dataset = CachedSurfingManeuverDataset(cache_dir=CACHE_VAL_DIR)\n",
        "test_dataset = CachedSurfingManeuverDataset(cache_dir=CACHE_TEST_DIR)\n",
        "\n",
        "# Create DataLoaders (no transform needed - videos are already preprocessed!)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {len(train_dataset)}\")\n",
        "print(f\"Validation set size: {len(val_dataset)}\")\n",
        "print(f\"Test set size: {len(test_dataset)}\")\n",
        "print(\"✓ Using CACHED dataset - Fast data loading enabled!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize Sample Videos\n",
        "\n",
        "Let's visualize a few frames from sample videos in the training set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_video_frames(dataloader, classes, n_videos=2, n_frames=4):\n",
        "    \"\"\"\n",
        "    Plot sample frames from videos in the dataset.\n",
        "    \"\"\"\n",
        "    videos, labels = next(iter(dataloader))\n",
        "    videos = videos[:n_videos]\n",
        "    labels = labels[:n_videos]\n",
        "    \n",
        "    # Denormalize for visualization\n",
        "    mean = torch.tensor(MEAN).view(1, 3, 1, 1, 1)\n",
        "    std = torch.tensor(STD).view(1, 3, 1, 1, 1)\n",
        "    videos = videos * std + mean\n",
        "    \n",
        "    fig, axes = plt.subplots(n_videos, n_frames, figsize=(15, 4 * n_videos))\n",
        "    if n_videos == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "    \n",
        "    for i in range(n_videos):\n",
        "        # Sample frames uniformly from the video\n",
        "        frame_indices = np.linspace(0, videos.shape[2] - 1, n_frames, dtype=int)\n",
        "        \n",
        "        for j, frame_idx in enumerate(frame_indices):\n",
        "            # Extract frame: (C, T, H, W) -> (H, W, C)\n",
        "            frame = videos[i, :, frame_idx, :, :].permute(1, 2, 0).numpy()\n",
        "            frame = np.clip(frame, 0, 1)\n",
        "            \n",
        "            axes[i, j].imshow(frame)\n",
        "            if j == 0:\n",
        "                axes[i, j].set_ylabel(classes[labels[i].item()], fontsize=12, fontweight='bold')\n",
        "            axes[i, j].set_title(f'Frame {frame_idx + 1}')\n",
        "            axes[i, j].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize samples\n",
        "if len(train_dataset) > 0:\n",
        "    plot_video_frames(train_loader, class_names, n_videos=2, n_frames=4)\n",
        "else:\n",
        "    print(\"No videos found in training set. Please check your data directory.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Loop\n",
        "\n",
        "Define a training function for video classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
        "    \"\"\"\n",
        "    Train for one epoch.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for batch_idx, (videos, labels) in enumerate(train_loader):\n",
        "        videos, labels = videos.to(device), labels.to(device)\n",
        "        \n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(videos)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Statistics\n",
        "        running_loss += loss.item() * videos.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        \n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print(f'  Batch [{batch_idx + 1}/{len(train_loader)}], '\n",
        "                  f'Loss: {loss.item():.4f}, Acc: {100. * correct / total:.2f}%')\n",
        "    \n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    epoch_acc = 100. * correct / total\n",
        "    \n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Validate the model.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for videos, labels in val_loader:\n",
        "            videos, labels = videos.to(device), labels.to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(videos)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            # Statistics\n",
        "            running_loss += loss.item() * videos.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "    \n",
        "    epoch_loss = running_loss / len(val_loader.dataset)\n",
        "    epoch_acc = 100. * correct / total\n",
        "    \n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def train_for_epochs(model, train_loader, val_loader, optimizer, criterion, device, epochs):\n",
        "    \"\"\"\n",
        "    Train the model for multiple epochs.\n",
        "    \"\"\"\n",
        "    history = {\n",
        "        'train': {'loss': [], 'accuracy': []},\n",
        "        'val': {'loss': [], 'accuracy': []}\n",
        "    }\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        print(f'\\nEpoch [{epoch + 1}/{epochs}]')\n",
        "        print('-' * 50)\n",
        "        \n",
        "        # Train\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "        \n",
        "        # Validate\n",
        "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
        "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "        \n",
        "        # Save history\n",
        "        history['train']['loss'].append(train_loss)\n",
        "        history['train']['accuracy'].append(train_acc)\n",
        "        history['val']['loss'].append(val_loss)\n",
        "        history['val']['accuracy'].append(val_acc)\n",
        "    \n",
        "    return history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Pretrained R3D Model\n",
        "\n",
        "Let's load R3D pretrained on Kinetics-400 (a large action recognition dataset with 400 classes) and adapt it for our surfing maneuver classification task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model save path\n",
        "MODEL_PATH = 'r3d_surfing_model.pth'\n",
        "\n",
        "# Check if saved model exists\n",
        "if os.path.exists(MODEL_PATH):\n",
        "    print(f\"Found saved model at '{MODEL_PATH}', loading...\")\n",
        "    \n",
        "    # Create model structure\n",
        "    model_transfer = r3d_18(weights=None)\n",
        "    model_transfer.fc = nn.Linear(512, NUM_CLASSES)\n",
        "    \n",
        "    # Load saved weights\n",
        "    model_transfer.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "    model_transfer = model_transfer.to(device)\n",
        "    model_transfer.eval()\n",
        "    \n",
        "    print(\"✓ Loaded saved model successfully!\")\n",
        "    print(f\"Total parameters: {sum(p.numel() for p in model_transfer.parameters()):,}\")\n",
        "    \n",
        "    # Create optimizer (for consistency, even though we won't train)\n",
        "    optimizer_transfer = optim.Adam(\n",
        "        model_transfer.fc.parameters(), \n",
        "        lr=LEARNING_RATE,\n",
        "        weight_decay=1e-4\n",
        "    )\n",
        "    \n",
        "else:\n",
        "    print(\"No saved model found, creating new model...\")\n",
        "    \n",
        "    # Load R3D with pretrained weights from Kinetics-400\n",
        "    weights = R3D_18_Weights.KINETICS400_V1\n",
        "    model_transfer = r3d_18(weights=weights)\n",
        "\n",
        "    # Freeze all layers first\n",
        "    for param in model_transfer.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Replace the final classification layer for our 4 classes\n",
        "    # R3D classifier is: fc (Linear layer with 512 input features -> 400 output classes)\n",
        "    # We need to replace this with our own layer\n",
        "    in_features = model_transfer.fc.in_features  # 512\n",
        "    model_transfer.fc = nn.Linear(in_features, NUM_CLASSES)\n",
        "\n",
        "    # Move to device\n",
        "    model_transfer = model_transfer.to(device)\n",
        "\n",
        "    # Optimizer - only optimize the final classifier layer\n",
        "    optimizer_transfer = optim.Adam(\n",
        "        model_transfer.fc.parameters(), \n",
        "        lr=LEARNING_RATE,\n",
        "        weight_decay=1e-4\n",
        "    )\n",
        "\n",
        "    print(\"R3D Model (pretrained on Kinetics-400):\")\n",
        "    print(f\"Total parameters: {sum(p.numel() for p in model_transfer.parameters()):,}\")\n",
        "    print(f\"Trainable parameters: {sum(p.numel() for p in model_transfer.parameters() if p.requires_grad):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train with transfer learning (only if model doesn't exist)\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    if len(train_dataset) > 0:\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"Training R3D with Transfer Learning...\")\n",
        "        print(\"=\"*50)\n",
        "        history_transfer = train_for_epochs(\n",
        "            model_transfer, \n",
        "            train_loader, \n",
        "            val_loader, \n",
        "            optimizer_transfer, \n",
        "            criterion, \n",
        "            device, \n",
        "            EPOCHS\n",
        "        )\n",
        "        \n",
        "        # Save the trained model\n",
        "        torch.save(model_transfer.state_dict(), MODEL_PATH)\n",
        "        print(f\"\\n✓ Model saved as '{MODEL_PATH}'\")\n",
        "    else:\n",
        "        print(\"Skipping training - no data found\")\n",
        "        print(\"Please update TRAIN_DATA_DIR and VAL_DATA_DIR with your dataset paths.\")\n",
        "else:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Using existing trained model, skipping training.\")\n",
        "    print(f\"Model loaded from: '{MODEL_PATH}'\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"\\nTo retrain from scratch, delete the model file and rerun this cell.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (Optional) Fine-tuning: Unfreeze More Layers\n",
        "\n",
        "After initial training, we can optionally unfreeze more layers for fine-tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to perform fine-tuning\n",
        "\n",
        "# # Unfreeze the last few layers for fine-tuning\n",
        "# # R3D structure: stem -> layer1 -> layer2 -> layer3 -> layer4 -> avgpool -> fc\n",
        "# for param in model_transfer.layer4.parameters():\n",
        "#     param.requires_grad = True\n",
        "\n",
        "# # Use a lower learning rate for fine-tuning\n",
        "# optimizer_finetune = optim.Adam(\n",
        "#     filter(lambda p: p.requires_grad, model_transfer.parameters()),\n",
        "#     lr=LEARNING_RATE * 0.1,\n",
        "#     weight_decay=1e-4\n",
        "# )\n",
        "\n",
        "# print(f\"Fine-tuning - Trainable parameters: {sum(p.numel() for p in model_transfer.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "# # Fine-tune for additional epochs\n",
        "# if len(train_dataset) > 0:\n",
        "#     print(\"\\n\" + \"=\"*50)\n",
        "#     print(\"Fine-tuning R3D...\")\n",
        "#     print(\"=\"*50)\n",
        "#     history_finetune = train_for_epochs(\n",
        "#         model_transfer, \n",
        "#         train_loader, \n",
        "#         val_loader, \n",
        "#         optimizer_finetune, \n",
        "#         criterion, \n",
        "#         device, \n",
        "#         EPOCHS // 2  # Fine-tune for fewer epochs\n",
        "#     )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot Training History\n",
        "\n",
        "Visualize the training and validation metrics over epochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "def plot_history(history, title):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    \n",
        "    # Plot loss\n",
        "    axes[0].plot(history['train']['loss'], label='Train Loss', marker='o')\n",
        "    axes[0].plot(history['val']['loss'], label='Val Loss', marker='s')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].set_title(f'{title} - Loss')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True)\n",
        "    \n",
        "    # Plot accuracy\n",
        "    axes[1].plot(history['train']['accuracy'], label='Train Accuracy', marker='o')\n",
        "    axes[1].plot(history['val']['accuracy'], label='Val Accuracy', marker='s')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('Accuracy (%)')\n",
        "    axes[1].set_title(f'{title} - Accuracy')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot transfer learning results\n",
        "if 'history_transfer' in locals():\n",
        "    plot_history(history_transfer, 'R3D Transfer Learning')\n",
        "    print(f\"\\nFinal Validation Accuracy: {history_transfer['val']['accuracy'][-1]:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inference Example\n",
        "\n",
        "Let's test the model on some test videos to evaluate final performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_video(model, video_tensor, device, class_names):\n",
        "    \"\"\"\n",
        "    Predict the class of a video.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        video_tensor = video_tensor.unsqueeze(0).to(device)  # Add batch dimension\n",
        "        output = model(video_tensor)\n",
        "        probabilities = torch.nn.functional.softmax(output, dim=1)\n",
        "        confidence, predicted = probabilities.max(1)\n",
        "        \n",
        "    return class_names[predicted.item()], confidence.item()\n",
        "\n",
        "\n",
        "# Test on TEST set\n",
        "if 'model_transfer' in locals() and len(test_dataset) > 0:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Testing on TEST videos\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    for i in range(min(5, len(test_dataset))):\n",
        "        video, true_label = test_dataset[i]\n",
        "        predicted_class, confidence = predict_video(model_transfer, video, device, class_names)\n",
        "        true_class = class_names[true_label]\n",
        "        \n",
        "        print(f\"\\nVideo {i+1}:\")\n",
        "        print(f\"  True label: {true_class}\")\n",
        "        print(f\"  Predicted: {predicted_class} (confidence: {confidence*100:.2f}%)\")\n",
        "        print(f\"  {'✓ Correct' if predicted_class == true_class else '✗ Incorrect'}\")\n",
        "    \n",
        "    # Evaluate on entire test set\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Full Test Set Evaluation\")\n",
        "    print(\"=\"*50)\n",
        "    test_loss, test_acc = validate(model_transfer, test_loader, criterion, device)\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### References\n",
        "\n",
        "- [R3D Paper: A Closer Look at Spatiotemporal Convolutions for Action Recognition](https://arxiv.org/abs/1711.11248)\n",
        "- [PyTorch Video Models Documentation](https://pytorch.org/vision/stable/models.html#video-classification)\n",
        "- [Kinetics-400 Dataset](https://deepmind.com/research/open-source/kinetics)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
